% refer to https://en.wikibooks.org/wiki/LaTeX/Glossary for acronyms and glossary entries

\newglossaryentry{ngram}{
    name={N-Gram Model},
    description={A type of probabilistic language model. It is based on the assumption that the probability of a word in a text depends only on the previous \(N-1\) words. Words' probability is computed by analysing the occurrence of N-grams, or any N words, in the training corpora. The archetype on which all language models have been developed}
}

\newglossaryentry{fm}{
    name={Foundational Model},
    description={An alternative name used for large language models. Introduced by the scientific community to emphasise the emergent behaviour created by scaling up the training corpora and the number of parameters. Just as large language models, foundational models are used in modern natural language processing on a variety of tasks with no fine-tuning or through prompt engineering}
}

\newacronym{ai}{AI}{Artificial Intelligence}

\newacronym{gml}{GML}{Graph Markup Language}

\newacronym{act}{ACT}{Adaptive Control of Thought}

\newacronym[
    shortplural={RAG},
    longplural={Retrieval Augmented Generation}
]{rag}{RAG}
{Retrieval Augmented Generation}

\newacronym[shortplural={LLMs},longplural={Large Language Models}]{llm}{LLM}{Large Language Model}

\newacronym[shortplural={GPTs},longplural={Generative Pre-Trained Transformers}]{gpt}{GPT}{Generative Pre-Trained Transformer}

\newacronym{nlp}{NLP}{Natural Language Processing}

\newacronym{cot}{CoT}{Chain-of-Thought}

\newacronym{rlhf}{RLHF}{Reinforcement Learning from Human Feedback}

\newacronym{rl}{RL}{Reinforcement Learning}

\newacronym{json}{JSON}{Javascript Object Notation}

\newacronym{ui}{UI}{User Interface}
